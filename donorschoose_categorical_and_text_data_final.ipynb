{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "from Wisard import Wisard\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get parameter from command line\n",
    "\n",
    "file = sys.argv[1]\n",
    "\n",
    "files = {\n",
    "    \"math\": \"matriz_Math_Science.csv\",\n",
    "    \"health\": \"matriz_Health_Sports.csv\",\n",
    "    \"warmth\": \"matriz_Warmth_Care_Hunger.csv\",\n",
    "    \"special\": \"matriz_Special_Needs.csv\",\n",
    "    \"history\": \"matriz_History_Civics.csv\",\n",
    "    \"music\": \"matriz_Music_The_Arts.csv\", \n",
    "    \"applied\": \"matriz_Applied_Learning.csv\",\n",
    "    #\"math\": \"matriz_Math_Science.csv\",\n",
    "}\n",
    "\n",
    "distribution = [1000, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_data = pd.read_csv(\"matriz_Applied_Learning.csv\", sep=';')\n",
    "#health_sports_data = pd.read_csv(\"matriz_Health_Sports.csv\", sep=';')\n",
    "#category_data = pd.read_csv(files[file], sep=';')\n",
    "\n",
    "train_data = pd.read_csv(\"train.csv\", sep=',')\n",
    "#test_data = pd.read_csv(test_file_path, sep=',')\n",
    "resources_data = pd.read_csv(\"resources.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>approved</th>\n",
       "      <th>shakespear</th>\n",
       "      <th>razkid</th>\n",
       "      <th>funfil</th>\n",
       "      <th>readalong</th>\n",
       "      <th>jungl</th>\n",
       "      <th>vowel</th>\n",
       "      <th>vr</th>\n",
       "      <th>reclin</th>\n",
       "      <th>...</th>\n",
       "      <th>profess</th>\n",
       "      <th>afterschool</th>\n",
       "      <th>repair</th>\n",
       "      <th>weak</th>\n",
       "      <th>medicin</th>\n",
       "      <th>sky</th>\n",
       "      <th>communitythi</th>\n",
       "      <th>string</th>\n",
       "      <th>blossom</th>\n",
       "      <th>recip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p063374</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p181781</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p225747</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p232007</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p037127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  approved  shakespear  razkid  funfil  readalong  jungl  vowel  vr  \\\n",
       "0  p063374         1           0       0       0          0      0      0   0   \n",
       "1  p181781         1           0       0       0          0      0      0   0   \n",
       "2  p225747         1           0       0       0          0      0      0   0   \n",
       "3  p232007         1           0       0       0          0      0      0   0   \n",
       "4  p037127         0           0       0       0          0      0      0   0   \n",
       "\n",
       "   reclin  ...    profess  afterschool  repair  weak  medicin  sky  \\\n",
       "0       0  ...          0            0       0     0        0    0   \n",
       "1       0  ...          0            0       0     0        0    0   \n",
       "2       0  ...          0            0       0     0        0    0   \n",
       "3       0  ...          0            0       0     0        0    0   \n",
       "4       0  ...          0            0       0     0        0    0   \n",
       "\n",
       "   communitythi  string  blossom  recip  \n",
       "0             0       0        0      0  \n",
       "1             0       0        0      0  \n",
       "2             0       0        0      0  \n",
       "3             0       0        0      0  \n",
       "4             0       0        0      0  \n",
       "\n",
       "[5 rows x 5002 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#health_sports_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16811\n",
      "16811\n"
     ]
    }
   ],
   "source": [
    "print(len(category_data))\n",
    "print(len(category_data[\"id\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenated = pd.concat([math_science_data[:500], health_sports_data[:500]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del math_science_data\n",
    "#del health_sports_data\n",
    "\n",
    "#concatenated = math_science_data\n",
    "concatenated = category_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>approved</th>\n",
       "      <th>shakespear</th>\n",
       "      <th>razkid</th>\n",
       "      <th>funfil</th>\n",
       "      <th>readalong</th>\n",
       "      <th>jungl</th>\n",
       "      <th>vowel</th>\n",
       "      <th>vr</th>\n",
       "      <th>reclin</th>\n",
       "      <th>...</th>\n",
       "      <th>profess</th>\n",
       "      <th>afterschool</th>\n",
       "      <th>repair</th>\n",
       "      <th>weak</th>\n",
       "      <th>medicin</th>\n",
       "      <th>sky</th>\n",
       "      <th>communitythi</th>\n",
       "      <th>string</th>\n",
       "      <th>blossom</th>\n",
       "      <th>recip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p063374</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p181781</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p225747</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p232007</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p037127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  approved  shakespear  razkid  funfil  readalong  jungl  vowel  vr  \\\n",
       "0  p063374         1           0       0       0          0      0      0   0   \n",
       "1  p181781         1           0       0       0          0      0      0   0   \n",
       "2  p225747         1           0       0       0          0      0      0   0   \n",
       "3  p232007         1           0       0       0          0      0      0   0   \n",
       "4  p037127         0           0       0       0          0      0      0   0   \n",
       "\n",
       "   reclin  ...    profess  afterschool  repair  weak  medicin  sky  \\\n",
       "0       0  ...          0            0       0     0        0    0   \n",
       "1       0  ...          0            0       0     0        0    0   \n",
       "2       0  ...          0            0       0     0        0    0   \n",
       "3       0  ...          0            0       0     0        0    0   \n",
       "4       0  ...          0            0       0     0        0    0   \n",
       "\n",
       "   communitythi  string  blossom  recip  \n",
       "0             0       0        0      0  \n",
       "1             0       0        0      0  \n",
       "2             0       0        0      0  \n",
       "3             0       0        0      0  \n",
       "4             0       0        0      0  \n",
       "\n",
       "[5 rows x 5002 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5002\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "words_list = list(concatenated.columns)\n",
    "print(len(words_list))\n",
    "words_list.remove(\"id\")\n",
    "words_list.remove(\"approved\")\n",
    "print(len(words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "None in concatenated[\"id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If fillna() is needs to be used, be sure to restrict it to all the fields but the \"id\".\n",
    "#concatenated = concatenated.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>approved</th>\n",
       "      <th>shakespear</th>\n",
       "      <th>razkid</th>\n",
       "      <th>funfil</th>\n",
       "      <th>readalong</th>\n",
       "      <th>jungl</th>\n",
       "      <th>vowel</th>\n",
       "      <th>vr</th>\n",
       "      <th>reclin</th>\n",
       "      <th>...</th>\n",
       "      <th>profess</th>\n",
       "      <th>afterschool</th>\n",
       "      <th>repair</th>\n",
       "      <th>weak</th>\n",
       "      <th>medicin</th>\n",
       "      <th>sky</th>\n",
       "      <th>communitythi</th>\n",
       "      <th>string</th>\n",
       "      <th>blossom</th>\n",
       "      <th>recip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p063374</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p181781</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p225747</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p232007</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p037127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  approved  shakespear  razkid  funfil  readalong  jungl  vowel  vr  \\\n",
       "0  p063374         1           0       0       0          0      0      0   0   \n",
       "1  p181781         1           0       0       0          0      0      0   0   \n",
       "2  p225747         1           0       0       0          0      0      0   0   \n",
       "3  p232007         1           0       0       0          0      0      0   0   \n",
       "4  p037127         0           0       0       0          0      0      0   0   \n",
       "\n",
       "   reclin  ...    profess  afterschool  repair  weak  medicin  sky  \\\n",
       "0       0  ...          0            0       0     0        0    0   \n",
       "1       0  ...          0            0       0     0        0    0   \n",
       "2       0  ...          0            0       0     0        0    0   \n",
       "3       0  ...          0            0       0     0        0    0   \n",
       "4       0  ...          0            0       0     0        0    0   \n",
       "\n",
       "   communitythi  string  blossom  recip  \n",
       "0             0       0        0      0  \n",
       "1             0       0        0      0  \n",
       "2             0       0        0      0  \n",
       "3             0       0        0      0  \n",
       "4             0       0        0      0  \n",
       "\n",
       "[5 rows x 5002 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(training_dataframe, resources_dataframe):\n",
    "    #print(training_dataframe.shape)\n",
    "    #print(resources_dataframe.shape)\n",
    "    \n",
    "    #\n",
    "    total_price = resources_dataframe.quantity * resources_dataframe.price\n",
    "    resources_dataframe[\"total_price\"] = total_price\n",
    "    \n",
    "    # dropping irrelevant columns\n",
    "    resources_dataframe = resources_dataframe.drop([\"description\", \"price\"], axis=1)\n",
    "    training_dataframe = training_dataframe.drop([\"teacher_id\"], axis=1)\n",
    "    \n",
    "    # grouping resources data by id\n",
    "    grouped_resources_dataframe = resources_dataframe.groupby(\"id\", as_index=False, sort=False).sum()\n",
    "    grouped_resources_dataframe\n",
    "    \n",
    "    # merging the two dataframes\n",
    "    cleaned_df = pd.merge(training_dataframe, grouped_resources_dataframe, how=\"inner\", on=[\"id\"])\n",
    "    \n",
    "    # splitting project categories\n",
    "    \n",
    "    cleaned_df[['category_1','category_2']] = cleaned_df['project_subject_categories'].str.replace(\", Care & Hunger\", \"\").str.split(', ', 3, expand=True)\n",
    "    #print(cleaned_df['project_subject_categories'].str.replace(\", Care & Hunger\", \"\").str.split(', ', 3, expand=True))\n",
    "    \n",
    "    cleaned_df[['subcategory_1','subcategory_2']] = cleaned_df['project_subject_subcategories'].str.replace(\", Care & Hunger\", \"\").str.split(', ', 3, expand=True)\n",
    "    \n",
    "    #cleaned_df[\"category_1\"] = cleaned_df[\"category_1\"].fillna(\"Not Informed\")\n",
    "    cleaned_df[\"category_2\"] = cleaned_df[\"category_2\"].fillna(\"Not Informed\")\n",
    "    cleaned_df[\"subcategory_2\"] = cleaned_df[\"subcategory_2\"].fillna(\"Not Informed\")\n",
    "    \n",
    "    cleaned_df[\"total_price_category\"] = pd.cut(\n",
    "        cleaned_df[\"total_price\"], \n",
    "        bins=[0,100,250,500,1000,16000], \n",
    "        labels=[\"0-100\",\"101-250\",\"251-500\",\"501-1000\",\">1000\"]\n",
    "    )\n",
    "    \n",
    "    cleaned_df[\"n_previous_projects\"] = pd.cut(\n",
    "        cleaned_df[\"teacher_number_of_previously_posted_projects\"],\n",
    "        bins=[-1,1,5,10,25,50,500],\n",
    "        labels=['0-1','2-5','6-10','11-25','26-50','51+']\n",
    "    )\n",
    "    \n",
    "    cleaned_df[\"project_submitted_datetime\"] = pd.to_datetime(cleaned_df['project_submitted_datetime'])\n",
    "    cleaned_df[\"month_\"] = cleaned_df['project_submitted_datetime'].dt.month\n",
    "    cleaned_df[\"quarter_\"] = cleaned_df['project_submitted_datetime'].dt.quarter\n",
    "    \n",
    "    cleaned_df[\"teacher_prefix\"] = cleaned_df[\"teacher_prefix\"].fillna(\"unknown\")\n",
    "    \n",
    "    cleaned_df[\"project_essay_1\"] = cleaned_df[\"project_essay_1\"].fillna(\"\")\n",
    "    cleaned_df[\"project_essay_2\"] = cleaned_df[\"project_essay_2\"].fillna(\"\")\n",
    "    cleaned_df[\"project_essay_3\"] = cleaned_df[\"project_essay_3\"].fillna(\"\")\n",
    "    cleaned_df[\"project_essay_4\"] = cleaned_df[\"project_essay_4\"].fillna(\"\")\n",
    "    \n",
    "    #cleaned_df[\"merged_essays\"] = cleaned_df['project_title'].astype(str) + \" \" + cleaned_df['project_essay_1'].astype(str) + \" \" + cleaned_df['project_essay_2'].astype(str) + \" \" + cleaned_df['project_essay_3'].astype(str) + \" \" + cleaned_df['project_essay_4'].astype(str)\n",
    "    cleaned_df[\"merged_essays\"] = cleaned_df['project_essay_1'].astype(str) + \" \" + cleaned_df['project_essay_2'].astype(str) + \" \" + cleaned_df['project_essay_3'].astype(str) + \" \" + cleaned_df['project_essay_4'].astype(str)\n",
    "    \n",
    "    # dropping more columns\n",
    "    cleaned_df = cleaned_df.drop([\n",
    "        \"project_submitted_datetime\", \n",
    "        \"project_essay_1\", \n",
    "        \"project_essay_2\", \n",
    "        \"project_essay_3\", \n",
    "        \"project_essay_4\",\n",
    "        \"quantity\",\n",
    "        \"total_price\",\n",
    "        \"teacher_number_of_previously_posted_projects\"], \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataframe shape:  (182080, 18)\n",
      "Concatenated shape:  (16811, 5002)\n",
      "Concatenated approved:  13827\n"
     ]
    }
   ],
   "source": [
    "training_dataframe = preprocess(train_data, resources_data)\n",
    "print(\"Training dataframe shape: \", training_dataframe.shape)\n",
    "print(\"Concatenated shape: \", concatenated.shape)\n",
    "print(\"Concatenated approved: \", concatenated[\"approved\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape:  (16811, 5019)\n"
     ]
    }
   ],
   "source": [
    "concatenated = pd.merge(training_dataframe, concatenated, how=\"inner\", on=[\"id\"])\n",
    "\n",
    "print(\"New shape: \", concatenated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = concatenated.sample(frac=1,random_state=3356) # shuffling the dataset\n",
    "#approved = train[train[\"project_is_approved\"] == 1][:1000] # gets the first 1000 approved\n",
    "#reproved = train[train[\"project_is_approved\"] == 0][:1000] # gets the first 1000 reproved\n",
    "\n",
    "approved = concatenated[concatenated[\"project_is_approved\"] == 1][:distribution[0]] # gets the first n approved\n",
    "reproved = concatenated[concatenated[\"project_is_approved\"] == 0][:distribution[1]] # gets the first n reproved\n",
    "\n",
    "training_set = pd.concat([approved, reproved])\n",
    "#training_set = training_set.sample(frac=1,random_state=3350) # shuffling the dataset\n",
    "test_set = concatenated.drop(training_set.index)\n",
    "training_set = training_set.reset_index(drop=True)\n",
    "test_set = test_set.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16811\n",
      "2000\n",
      "14811\n"
     ]
    }
   ],
   "source": [
    "print(len(concatenated))\n",
    "print(len(training_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I know, wrong name...\n",
    "def convert_to_bits_string(dataframe, words_list):\n",
    "    print(\"Converting dataframe of shape \", dataframe.shape, \" to a list of binary values.\")\n",
    "    \n",
    "    project_grade_category_mapping = { # 4\n",
    "        'Grades PreK-2':  \"1000\", \n",
    "        'Grades 3-5':     \"1100\", \n",
    "        'Grades 6-8':     \"1110\", \n",
    "        'Grades 9-12':    \"1111\"\n",
    "    }\n",
    "\n",
    "    teacher_prefix_mapping = { # 6\n",
    "        'Ms.':     \"100000\", \n",
    "        'Mrs.':    \"110000\", \n",
    "        'Mr.':     \"111000\", \n",
    "        'Teacher': \"111100\", \n",
    "        'Dr.':     \"111110\", \n",
    "        'unknown': \"111111\"\n",
    "    }\n",
    "\n",
    "    n_previous_projects_mapping = { # 6\n",
    "        '0-1':     \"100000\",\n",
    "        '2-5':     \"110000\",\n",
    "        '6-10':    \"111000\",\n",
    "        '11-25':   \"111100\",\n",
    "        '26-50':   \"111110\",\n",
    "        '51+':     \"111111\"\n",
    "    }\n",
    "\n",
    "    total_price_category_mapping = { # 6\n",
    "        \"0-100\":     \"100000\",\n",
    "        \"101-250\":   \"110000\",\n",
    "        \"251-500\":   \"111000\",\n",
    "        \"501-1000\":  \"111100\",\n",
    "        \">1000\":     \"111110\"\n",
    "    }\n",
    "    \n",
    "    month_mapping = { # 12\n",
    "        \"1\":  \"100000000000\",\n",
    "        \"2\":  \"110000000000\",\n",
    "        \"3\":  \"111000000000\",\n",
    "        \"4\":  \"111100000000\",\n",
    "        \"5\":  \"111110000000\",\n",
    "        \"6\":  \"111111000000\",\n",
    "        \"7\":  \"111111100000\",\n",
    "        \"8\":  \"111111110000\",\n",
    "        \"9\":  \"111111111000\",\n",
    "        \"10\": \"111111111100\",\n",
    "        \"11\": \"111111111110\",\n",
    "        \"12\": \"111111111111\"\n",
    "    }\n",
    "    \n",
    "    quarter_mapping = { # 4\n",
    "        \"1\":\"1000\",\n",
    "        \"2\":\"1100\",\n",
    "        \"3\":\"1110\",\n",
    "        \"4\":\"1111\"\n",
    "    }\n",
    "    \n",
    "    category_mapping = { # 10\n",
    "        \"Not Informed\":          \"1000000000\",\n",
    "        \"Applied Learning\":      \"1100000000\",\n",
    "        \"Health & Sports\":       \"1110000000\",\n",
    "        \"History & Civics\":      \"1111000000\",\n",
    "        \"Literacy & Language\":   \"1111100000\",\n",
    "        \"Math & Science\":        \"1111110000\",\n",
    "        \"Music & The Arts\":      \"1111111000\",\n",
    "        \"Special Needs\":         \"1111111100\",\n",
    "        \"Warmth\":                \"1111111110\",\n",
    "        \"Care & Hunger\":         \"1111111110\", # Equals to warmth, because they are the same thing\n",
    "    }\n",
    "    \n",
    "    subcategory_mapping = { # 30\n",
    "        \"Not Informed\":          \"100000000000000000000000000000\",\n",
    "        \"Literacy\":              \"110000000000000000000000000000\",\n",
    "        \"Performing Arts\":       \"111000000000000000000000000000\",\n",
    "        \"Applied Sciences\":      \"111100000000000000000000000000\",\n",
    "        \"Health & Wellness\":     \"111110000000000000000000000000\",\n",
    "        \"Character Education\":   \"111111000000000000000000000000\",\n",
    "        \"Early Development\":     \"111111100000000000000000000000\",\n",
    "        \"Mathematics\":           \"111111110000000000000000000000\",\n",
    "        \"Literature & Writing\":  \"111111111000000000000000000000\",\n",
    "        \"Special Needs\":         \"111111111100000000000000000000\", \n",
    "        \"ESL\":                   \"111111111110000000000000000000\", \n",
    "        \"Health & Life Science\": \"111111111111000000000000000000\", \n",
    "        \"College & Career Prep\": \"111111111111100000000000000000\", \n",
    "        \"Environmental Science\": \"111111111111110000000000000000\", \n",
    "        \"Other\":                 \"111111111111111000000000000000\", \n",
    "        \"Music\":                 \"111111111111111100000000000000\", \n",
    "        \"Visual Arts\":           \"111111111111111110000000000000\", \n",
    "        \"History & Geography\":   \"111111111111111111000000000000\", \n",
    "        \"Gym & Fitness\":         \"111111111111111111100000000000\", \n",
    "        \"Warmth\":                \"111111111111111111110000000000\", \n",
    "        \"Extracurricular\":       \"111111111111111111111000000000\", \n",
    "        \"Team Sports\":           \"111111111111111111111100000000\", \n",
    "        \"Social Sciences\":       \"111111111111111111111110000000\", \n",
    "        \"Foreign Languages\":     \"111111111111111111111111000000\", \n",
    "        \"Parent Involvement\":    \"111111111111111111111111100000\", \n",
    "        \"Nutrition Education\":   \"111111111111111111111111110000\", \n",
    "        \"Community Service\":     \"111111111111111111111111111000\", \n",
    "        \"Financial Literacy\":    \"111111111111111111111111111100\", \n",
    "        \"Civics & Government\":   \"111111111111111111111111111110\", \n",
    "        \"Economics\":             \"111111111111111111111111111111\", \n",
    "        \n",
    "    }\n",
    "    \n",
    "    school_state_mapping = { # 50\n",
    "        'NV':\"10000000000000000000000000000000000000000000000000\", \n",
    "        'GA':\"11000000000000000000000000000000000000000000000000\", \n",
    "        'UT':\"11100000000000000000000000000000000000000000000000\", \n",
    "        'NC':\"11110000000000000000000000000000000000000000000000\", \n",
    "        'CA':\"11111000000000000000000000000000000000000000000000\", \n",
    "        'DE':\"11111100000000000000000000000000000000000000000000\", \n",
    "        'MO':\"11111110000000000000000000000000000000000000000000\", \n",
    "        'SC':\"11111111000000000000000000000000000000000000000000\", \n",
    "        'IN':\"11111111100000000000000000000000000000000000000000\", \n",
    "        'IL':\"11111111110000000000000000000000000000000000000000\", \n",
    "        'VA':\"11111111111000000000000000000000000000000000000000\",\n",
    "        'PA':\"11111111111100000000000000000000000000000000000000\", \n",
    "        'NY':\"11111111111110000000000000000000000000000000000000\", \n",
    "        'FL':\"11111111111111000000000000000000000000000000000000\", \n",
    "        'NJ':\"11111111111111100000000000000000000000000000000000\", \n",
    "        'TX':\"11111111111111110000000000000000000000000000000000\", \n",
    "        'LA':\"11111111111111111000000000000000000000000000000000\", \n",
    "        'ID':\"11111111111111111100000000000000000000000000000000\", \n",
    "        'OH':\"11111111111111111110000000000000000000000000000000\", \n",
    "        'OR':\"11111111111111111111000000000000000000000000000000\", \n",
    "        'MD':\"11111111111111111111100000000000000000000000000000\", \n",
    "        'WA':\"11111111111111111111110000000000000000000000000000\",\n",
    "        'MA':\"11111111111111111111111000000000000000000000000000\", \n",
    "        'KY':\"11111111111111111111111100000000000000000000000000\", \n",
    "        'AZ':\"11111111111111111111111110000000000000000000000000\", \n",
    "        'MI':\"11111111111111111111111111000000000000000000000000\", \n",
    "        'CT':\"11111111111111111111111111100000000000000000000000\", \n",
    "        'AR':\"11111111111111111111111111110000000000000000000000\", \n",
    "        'WV':\"11111111111111111111111111111000000000000000000000\", \n",
    "        'NM':\"11111111111111111111111111111100000000000000000000\", \n",
    "        'WI':\"11111111111111111111111111111110000000000000000000\", \n",
    "        'MN':\"11111111111111111111111111111111000000000000000000\", \n",
    "        'OK':\"11111111111111111111111111111111100000000000000000\",\n",
    "        'AL':\"11111111111111111111111111111111110000000000000000\", \n",
    "        'TN':\"11111111111111111111111111111111111000000000000000\", \n",
    "        'IA':\"11111111111111111111111111111111111100000000000000\", \n",
    "        'KS':\"11111111111111111111111111111111111110000000000000\", \n",
    "        'CO':\"11111111111111111111111111111111111111000000000000\", \n",
    "        'DC':\"11111111111111111111111111111111111111100000000000\", \n",
    "        'WY':\"11111111111111111111111111111111111111110000000000\", \n",
    "        'NH':\"11111111111111111111111111111111111111111000000000\", \n",
    "        'HI':\"11111111111111111111111111111111111111111100000000\", \n",
    "        'SD':\"11111111111111111111111111111111111111111110000000\", \n",
    "        'MT':\"11111111111111111111111111111111111111111111000000\",\n",
    "        'MS':\"11111111111111111111111111111111111111111111100000\", \n",
    "        'RI':\"11111111111111111111111111111111111111111111110000\", \n",
    "        'VT':\"11111111111111111111111111111111111111111111111000\", \n",
    "        'ME':\"11111111111111111111111111111111111111111111111100\", \n",
    "        'NE':\"11111111111111111111111111111111111111111111111110\", \n",
    "        'AK':\"11111111111111111111111111111111111111111111111111\", \n",
    "        'ND':\"00000000000000000000000000000000000000000000000000\"\n",
    "    }\n",
    "    \n",
    "    combined_input_and_expected_output = []\n",
    "    input_list = []\n",
    "    expected_output_list = []\n",
    "    \n",
    "    n = 0\n",
    "    for index, row in dataframe.iterrows():\n",
    "        #print(row)\n",
    "        #if n >= 10:\n",
    "        #    break\n",
    "        #n = n + 1\n",
    "        \n",
    "        # total 128 bits\n",
    "        bits_string = \"\"\n",
    "        bits_string = project_grade_category_mapping[row[\"project_grade_category\"]]\n",
    "        bits_string = bits_string + teacher_prefix_mapping[row[\"teacher_prefix\"]]\n",
    "        bits_string = bits_string + n_previous_projects_mapping[row[\"n_previous_projects\"]]\n",
    "        bits_string = bits_string + total_price_category_mapping[row[\"total_price_category\"]]\n",
    "        \n",
    "        bits_string = bits_string + month_mapping[str(row[\"month_\"])]\n",
    "        bits_string = bits_string + quarter_mapping[str(row[\"quarter_\"])]\n",
    "        bits_string = bits_string + category_mapping[row[\"category_1\"]]\n",
    "        bits_string = bits_string + category_mapping[row[\"category_2\"]] # perhaps it is possible to ignore this one\n",
    "        \n",
    "        bits_string = bits_string + subcategory_mapping[row[\"subcategory_1\"]]\n",
    "        bits_string = bits_string + subcategory_mapping[row[\"subcategory_2\"]]\n",
    "        bits_string = bits_string + school_state_mapping[row[\"school_state\"]]\n",
    "        \n",
    "        bit_int_list = [int(c) for c in bits_string]\n",
    "        expected_output = str(row[\"project_is_approved\"])\n",
    "        \n",
    "        #\n",
    "        words_bits = row[words_list].values.tolist()\n",
    "        \n",
    "        #print(words_bits)\n",
    "        \n",
    "        bit_int_list.extend(words_bits)\n",
    "        \n",
    "        input_list.append(bit_int_list)\n",
    "        expected_output_list.append(expected_output)\n",
    "        \n",
    "        combined_input_and_expected_output.append([bit_int_list, expected_output])\n",
    "        \n",
    "    return input_list, expected_output_list, combined_input_and_expected_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting dataframe of shape  (2000, 5019)  to a list of binary values.\n",
      "Converting dataframe of shape  (14811, 5019)  to a list of binary values.\n"
     ]
    }
   ],
   "source": [
    "training_patterns, training_data_expected_output, training_data_combined = convert_to_bits_string(training_set, words_list)\n",
    "test_patterns, test_data_expected_output, test_data_combined = convert_to_bits_string(test_set, words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "5168\n"
     ]
    }
   ],
   "source": [
    "print(len(training_patterns))\n",
    "print(len(training_patterns[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(training_set[\"approved\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training_set_expected_values = training_set[\"approved\"]\n",
    "#training_set = training_set.drop([\"approved\", \"id\"], axis=1)\n",
    "#training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(training_set_expected_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_set_expected_values = test_set[\"approved\"]\n",
    "#test_set = test_set.drop([\"approved\", \"id\"], axis=1)\n",
    "#test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(concatenated.columns))\n",
    "\n",
    "# This conversion takes a lot of time.\n",
    "#training_set = training_set[list(training_set.columns)].applymap(np.int64)\n",
    "#test_set = test_set[list(test_set.columns)].applymap(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_data_combined = []\\ntraining_patterns = []\\ntraining_data_expected_output = []\\nfor i in range(len(training_set)):\\n    #print(str(expected_values.loc[[i]].values[0]))\\n    training_data_combined.append([training_set.loc[[i]].values.tolist()[0], str(training_set_expected_values.loc[[i]].values[0])])\\n    training_patterns.append(training_set.loc[[i]].values.tolist()[0])\\n    training_data_expected_output.append(str(training_set_expected_values.loc[[i]].values[0]))\\n\\nprint(len(training_data_combined))\\nprint(len(training_data_combined[0]))\\nprint(len(training_patterns))\\nprint(len(training_patterns[0]))\\n#training_patterns[0]\\n#expected_output'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"training_data_combined = []\n",
    "training_patterns = []\n",
    "training_data_expected_output = []\n",
    "for i in range(len(training_set)):\n",
    "    #print(str(expected_values.loc[[i]].values[0]))\n",
    "    training_data_combined.append([training_set.loc[[i]].values.tolist()[0], str(training_set_expected_values.loc[[i]].values[0])])\n",
    "    training_patterns.append(training_set.loc[[i]].values.tolist()[0])\n",
    "    training_data_expected_output.append(str(training_set_expected_values.loc[[i]].values[0]))\n",
    "\n",
    "print(len(training_data_combined))\n",
    "print(len(training_data_combined[0]))\n",
    "print(len(training_patterns))\n",
    "print(len(training_patterns[0]))\n",
    "#training_patterns[0]\n",
    "#expected_output\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_data_combined = []\\ntest_patterns = []\\ntest_data_expected_output = []\\nfor i in range(len(test_set)):\\n    #print(str(expected_values.loc[[i]].values[0]))\\n    test_data_combined.append([test_set.loc[[i]].values.tolist()[0], str(test_set_expected_values.loc[[i]].values[0])])\\n    test_patterns.append(test_set.loc[[i]].values.tolist()[0])\\n    test_data_expected_output.append(str(test_set_expected_values.loc[[i]].values[0]))\\n\\nprint(len(test_data_combined))\\nprint(len(test_data_combined[0]))\\nprint(len(test_patterns))\\nprint(len(test_patterns[0]))'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"test_data_combined = []\n",
    "test_patterns = []\n",
    "test_data_expected_output = []\n",
    "for i in range(len(test_set)):\n",
    "    #print(str(expected_values.loc[[i]].values[0]))\n",
    "    test_data_combined.append([test_set.loc[[i]].values.tolist()[0], str(test_set_expected_values.loc[[i]].values[0])])\n",
    "    test_patterns.append(test_set.loc[[i]].values.tolist()[0])\n",
    "    test_data_expected_output.append(str(test_set_expected_values.loc[[i]].values[0]))\n",
    "\n",
    "print(len(test_data_combined))\n",
    "print(len(test_data_combined[0]))\n",
    "print(len(test_patterns))\n",
    "print(len(test_patterns[0]))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wann = Wisard(2, 3546, False)\n",
    "#wann.train(training_patterns, training_data_expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(wann.discriminators[0].memory)\n",
    "\n",
    "def evaluate_performance(wann, test_data_combined):\n",
    "    #print(\"Number of observations: \", test_data_combined)\n",
    "    correct_predictions = 0\n",
    "    wrong_predictions = 0\n",
    "    zeros_predicted = 0\n",
    "    ones_predicted = 0\n",
    "    zeros_correct = 0\n",
    "    ones_correct = 0\n",
    "    zeros_wrong = 0\n",
    "    ones_wrong = 0\n",
    "    total_ties = 0\n",
    "    ones_total_ties = 0\n",
    "    zeros_total_ties = 0\n",
    "    avg_time = time.time()\n",
    "    \n",
    "    for combined in test_data_combined:\n",
    "        prediction, tie = wann.predict(combined[0])\n",
    "        prediction = prediction[\"class\"]\n",
    "        \n",
    "        if prediction == \"0\":\n",
    "            #print(\"Prediction: \", prediction[0], combined)\n",
    "            zeros_predicted = zeros_predicted + 1\n",
    "        elif prediction == \"1\":\n",
    "            ones_predicted = ones_predicted + 1\n",
    "        #print(prediction)\n",
    "        \n",
    "        expected = combined[1]\n",
    "        #print(prediction, expected)\n",
    "        if prediction == expected:\n",
    "            #print(\"Correct!\")\n",
    "            correct_predictions = correct_predictions + 1\n",
    "            \n",
    "            if prediction == \"0\":\n",
    "                zeros_correct = zeros_correct + 1\n",
    "            elif prediction == \"1\":\n",
    "                ones_correct = ones_correct + 1\n",
    "        else:\n",
    "            wrong_predictions = wrong_predictions + 1\n",
    "            \n",
    "            if prediction == \"0\":\n",
    "                zeros_wrong = zeros_wrong + 1\n",
    "            elif prediction == \"1\":\n",
    "                ones_wrong = ones_wrong + 1\n",
    "                \n",
    "        if tie:\n",
    "            total_ties = total_ties + 1\n",
    "            if expected == \"0\":\n",
    "                zeros_total_ties = zeros_total_ties + 1\n",
    "            elif expected == \"1\":\n",
    "                ones_total_ties = ones_total_ties + 1\n",
    "                \n",
    "    avg_time = float(time.time() - avg_time) / float(len(test_data_combined)) \n",
    "    \n",
    "    print(\"Number of observations: \", len(test_data_combined))\n",
    "    print(\"Predicted correctly: \", correct_predictions)\n",
    "    print(\"Predicted wrongly: \", wrong_predictions)\n",
    "    print(\"Predicted zeros: \", zeros_predicted)\n",
    "    print(\"Predicted ones: \", ones_predicted)\n",
    "    print(\"Zeros correct: \", zeros_correct)\n",
    "    print(\"Ones correct: \", ones_correct)\n",
    "    print(\"Zeros wrong: \", zeros_wrong)\n",
    "    print(\"Ones Wrong: \", ones_wrong)\n",
    "    print(\"Total ties: \", total_ties)\n",
    "    print(\"Zeros total ties: \", zeros_total_ties)\n",
    "    print(\"Ones total ties: \", ones_total_ties)\n",
    "    print(\"Avg. Time: \", avg_time, \" seconds.\")\n",
    "    return correct_predictions, [\n",
    "        len(test_data_combined), correct_predictions, wrong_predictions, zeros_predicted, ones_predicted,\n",
    "        zeros_correct, ones_correct, zeros_wrong, ones_wrong, total_ties, zeros_total_ties, ones_total_ties,\n",
    "        avg_time\n",
    "    ]\n",
    "\n",
    "#for i in range(len(training_patterns)):\n",
    "#    print(\"Result: \", wann.predict(training_patterns[i]), \"Expected: \", expected_output[i], \"\\n\")\n",
    "\n",
    "#evaluate_performance(wann, training_data_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_performance(wann, test_data_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wann = Wisard(2, 3546, True)\n",
    "#wann.train(training_patterns, training_data_expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_performance(wann, training_data_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_performance(wann, test_data_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_input, expected_output, tuple_size = 2, bleaching = False):\n",
    "    wann = Wisard(tuple_size, 3546, bleaching)\n",
    "    wann.train(training_input, expected_output)\n",
    "    return wann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(training_set_distribuitions, tuple_sizes, training_input, expected_output, training_combined, test_input, test_expected_output, test_combined, training_set, test_set, bleaching_mode = [False]):\n",
    "    output_file = \"insights/results_experiment_combined\" + datetime.now().strftime('%Y%m%d%H%M%S') + \".csv\"\n",
    "    file = open(output_file, \"w\")\n",
    "    file.write(\"data_distribution;tuple_size;bleaching_active;total_training_time;avg_in_sample_evaluation_time;total_in_sample_evaluation_time;avg_out_sample_evaluation_time;total_out_sample_evaluation_time;total_training_data;total_correct_training;\" +\n",
    "               \"percent_correct_training;total_approved_training;correctly_approved_training;wrongly_approved_training;\" +\n",
    "               \"percent_approved_correctly_training;total_reproved_training;correctly_reproved_training;\" +\n",
    "               \"wrongly_reproved_training;percent_reproved_correctly_training;total_test_data;total_correct_test;\" +\n",
    "               \"percent_correct_test;total_approved_test;correctly_approved_test;wrongly_approved_test;\" +\n",
    "               \"percent_approved_correctly_test;total_reproved_test;correctly_reproved_test;\" +\n",
    "               \"wrongly_reproved_test;percent_reproved_correctly_test;total_ties;ties_for_zeros;ties_for_ones\\n\"\n",
    "              )\n",
    "    file.close()\n",
    "    \n",
    "    training_set_distribuition = training_set_distribuitions[0]\n",
    "    \n",
    "    \n",
    "    print(\"\\nTraining with a training set distribution of \", \n",
    "          training_set_distribuition[0], training_set_distribuition[1],\n",
    "          \" for approved and repproved, respectively.\\n\")\n",
    "    #training_input, expected_output, training_combined, test_input, test_expected_output, test_combined, training_set, test_set = getData(training_set_distribuition[0], training_set_distribuition[1])\n",
    "\n",
    "    for a_tuple_size in tuple_sizes:\n",
    "        print(\"Training with a tupple of size: \", a_tuple_size)\n",
    "\n",
    "        for bleaching in bleaching_mode:\n",
    "            print(\"Bleaching is set to: \", bleaching, \"\\n\")\n",
    "\n",
    "            training_time = time.time()\n",
    "            wann = train(training_input, expected_output, a_tuple_size, bleaching)\n",
    "            training_time = time.time() - training_time\n",
    "\n",
    "            in_sample_evaluation_time = time.time()\n",
    "            in_sample_performance, in_sample_additional_info =  evaluate_performance(wann, training_combined)\n",
    "            in_sample_evaluation_time = time.time() - in_sample_evaluation_time\n",
    "\n",
    "            # Evaluates Guilherme's wisard implementation\n",
    "            print(\"In-sample performance: \", float(in_sample_performance) / float(len(training_combined)))\n",
    "            print(\"Ones distribution: \", float(training_set[\"project_is_approved\"].sum()) / float(len(training_set[\"project_is_approved\"])))\n",
    "            print(\"Ones: \", training_set[\"project_is_approved\"].sum(), \"Zeros: \", training_set[\"project_is_approved\"].sum() - len(training_set[\"project_is_approved\"]))\n",
    "            print(\"\\n\")\n",
    "\n",
    "            out_sample_evaluation_time = time.time()\n",
    "            out_sample_performance, out_sample_additional_info =  evaluate_performance(wann, test_combined)\n",
    "            out_sample_evaluation_time = time.time() - out_sample_evaluation_time\n",
    "\n",
    "            print(\"Expected out-sample performance: \", float(out_sample_performance) / float(len(test_combined)))\n",
    "            print(\"Ones distribution: \", float(test_set[\"project_is_approved\"].sum()) / float(len(test_set[\"project_is_approved\"])))\n",
    "            print(\"Ones: \", test_set[\"project_is_approved\"].sum(), \"Zeros: \", (test_set[\"project_is_approved\"].sum() - len(test_set[\"project_is_approved\"])), \"\\n\\n\")\n",
    "\n",
    "            line = \"\"\n",
    "            line_contents = [\n",
    "                # training / test\n",
    "                str(training_set_distribuition[0]) + \"/\" + str(training_set_distribuition[1]) + \";\",\n",
    "                # tuple size\n",
    "                str(a_tuple_size) + \";\",\n",
    "                # bleaching active or not\n",
    "                str(bleaching) + \";\",\n",
    "                # training time in seconds\n",
    "                str(training_time) + \";\",\n",
    "                # in sample pattern average evaluation time in seconds\n",
    "                str(in_sample_additional_info[12]) + \";\",\n",
    "                # in sample total evaluation time in seconds\n",
    "                str(in_sample_evaluation_time) + \";\",\n",
    "                # out sample pattern average evaluation time in seconds\n",
    "                str(out_sample_additional_info[12]) + \";\",\n",
    "                # out sample total evaluation time in seconds\n",
    "                str(out_sample_evaluation_time) + \";\",\n",
    "\n",
    "                # total traning observations\n",
    "                str(training_set_distribuition[0] + training_set_distribuition[1]) + \";\",\n",
    "                # total of correct prediction in the training dataset\n",
    "                str(in_sample_performance) + \";\",\n",
    "                # percentage of right answers\n",
    "                str(float(in_sample_performance) / float(len(training_combined))) + \";\",\n",
    "                # total approved in the training dataset\n",
    "                str(training_set[\"project_is_approved\"].sum()) + \";\",\n",
    "                # total approved correctly predicted in the training dataset\n",
    "                str(in_sample_additional_info[6]) + \";\",\n",
    "                # total approved wrongly predicted in the training dataset\n",
    "                str(in_sample_additional_info[8]) + \";\",\n",
    "                # percentage of approved projects predicted correctly in the training dataset\n",
    "                str(float(in_sample_additional_info[6]) / float(training_set[\"project_is_approved\"].sum())) + \";\",\n",
    "                # total reproved in the training dataset\n",
    "                str((training_set[\"project_is_approved\"].sum() - len(training_set[\"project_is_approved\"])) * -1) + \";\",\n",
    "                # total reproved correctly predicted in the training dataset\n",
    "                str(in_sample_additional_info[5]) + \";\",\n",
    "                # total reproved wrongly predicted in the training dataset\n",
    "                str(in_sample_additional_info[7]) + \";\",\n",
    "                # percentage of reproved projects predicted correctly in the training dataset\n",
    "                str(float(in_sample_additional_info[5]) / float((training_set[\"project_is_approved\"].sum() - len(training_set[\"project_is_approved\"])) * -1)) + \";\",\n",
    "\n",
    "\n",
    "                # total test observations\n",
    "                str(len(test_set[\"project_is_approved\"])) + \";\",\n",
    "                # total of correct prediction in the test dataset\n",
    "                str(out_sample_performance) + \";\",\n",
    "                # percentage of right answers\n",
    "                str(float(out_sample_performance) / float(len(test_combined))) + \";\",\n",
    "                # total approved in the test dataset\n",
    "                str(test_set[\"project_is_approved\"].sum()) + \";\",\n",
    "                # total approved correctly predicted in the test dataset\n",
    "                str(out_sample_additional_info[6]) + \";\",\n",
    "                # total approved wrongly predicted in the test dataset\n",
    "                str(out_sample_additional_info[8]) + \";\",\n",
    "                # percentage of approved projects predicted correctly in the test dataset\n",
    "                str(float(out_sample_additional_info[6]) / float(test_set[\"project_is_approved\"].sum())) + \";\",\n",
    "                # total reproved in the test dataset\n",
    "                str((test_set[\"project_is_approved\"].sum() - len(test_set[\"project_is_approved\"])) * -1) + \";\",\n",
    "                # total reproved correctly predicted in the training dataset\n",
    "                str(out_sample_additional_info[5]) + \";\",\n",
    "                # total reproved wrongly predicted in the training dataset\n",
    "                str(out_sample_additional_info[7]) + \";\",\n",
    "                # percentage of reproved projects predicted correctly in the training dataset\n",
    "                str(float(out_sample_additional_info[5]) / float((test_set[\"project_is_approved\"].sum() - len(test_set[\"project_is_approved\"])) * -1)) + \";\",\n",
    "\n",
    "                # total ties\n",
    "                str(out_sample_additional_info[9]) + \";\",\n",
    "                # total ties when prediction should have been zero\n",
    "                str(out_sample_additional_info[10]) + \";\",\n",
    "                # total ties when prediction should have been one\n",
    "                str(out_sample_additional_info[11]) + \";\",\n",
    "\n",
    "                \"\\n\",\n",
    "                #str() + \";\",\n",
    "            ]\n",
    "\n",
    "            for content in line_contents:\n",
    "                #print(content)\n",
    "                line = line + content\n",
    "\n",
    "            print(line)\n",
    "\n",
    "            file = open(output_file, \"a+\")\n",
    "            file.write(line)\n",
    "            file.close()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with a training set distribution of  1000 1000  for approved and repproved, respectively.\n",
      "\n",
      "Training with a tupple of size:  2\n",
      "Bleaching is set to:  False \n",
      "\n",
      "Number of classes being trained: 2\n",
      "dict_keys(['0', '1'])\n",
      "['0', '1']\n",
      "Number of training samples for class 0: 1000\n",
      "Number of training samples for class 1: 1000\n",
      "Number of observations:  2000\n",
      "Predicted correctly:  1655\n",
      "Predicted wrongly:  345\n",
      "Predicted zeros:  1345\n",
      "Predicted ones:  655\n",
      "Zeros correct:  1000\n",
      "Ones correct:  655\n",
      "Zeros wrong:  345\n",
      "Ones Wrong:  0\n",
      "Total ties:  858\n",
      "Zeros total ties:  513\n",
      "Ones total ties:  345\n",
      "Avg. Time:  0.023990033388137818  seconds.\n",
      "In-sample performance:  0.8275\n",
      "Ones distribution:  0.5\n",
      "Ones:  1000 Zeros:  -1000\n",
      "\n",
      "\n",
      "Number of observations:  14811\n",
      "Predicted correctly:  6145\n",
      "Predicted wrongly:  8666\n",
      "Predicted zeros:  9452\n",
      "Predicted ones:  5359\n",
      "Zeros correct:  1385\n",
      "Ones correct:  4760\n",
      "Zeros wrong:  8067\n",
      "Ones Wrong:  599\n",
      "Total ties:  6218\n",
      "Zeros total ties:  954\n",
      "Ones total ties:  5264\n",
      "Avg. Time:  0.024250019845002475  seconds.\n",
      "Expected out-sample performance:  0.4148943352913375\n",
      "Ones distribution:  0.8660455067179799\n",
      "Ones:  12827 Zeros:  -1984 \n",
      "\n",
      "\n",
      "1000/1000;2;False;29.302193880081177;0.023990033388137818;47.98080778121948;0.024250019845002475;359.16818261146545;2000;1655;0.8275;1000;655;0;0.655;1000;1000;345;1.0;14811;6145;0.4148943352913375;12827;4760;599;0.37109222733296954;1984;1385;8067;0.6980846774193549;6218;954;5264;\n",
      "\n",
      "Bleaching is set to:  True \n",
      "\n",
      "Number of classes being trained: 2\n",
      "dict_keys(['0', '1'])\n",
      "['0', '1']\n",
      "Number of training samples for class 0: 1000\n",
      "Number of training samples for class 1: 1000\n",
      "Number of observations:  2000\n",
      "Predicted correctly:  1761\n",
      "Predicted wrongly:  239\n",
      "Predicted zeros:  881\n",
      "Predicted ones:  1119\n",
      "Zeros correct:  821\n",
      "Ones correct:  940\n",
      "Zeros wrong:  60\n",
      "Ones Wrong:  179\n",
      "Total ties:  0\n",
      "Zeros total ties:  0\n",
      "Ones total ties:  0\n",
      "Avg. Time:  0.04612355124950409  seconds.\n",
      "In-sample performance:  0.8805\n",
      "Ones distribution:  0.5\n",
      "Ones:  1000 Zeros:  -1000\n",
      "\n",
      "\n",
      "Number of observations:  14811\n",
      "Predicted correctly:  8732\n",
      "Predicted wrongly:  6079\n",
      "Predicted zeros:  5801\n",
      "Predicted ones:  9010\n",
      "Zeros correct:  853\n",
      "Ones correct:  7879\n",
      "Zeros wrong:  4948\n",
      "Ones Wrong:  1131\n",
      "Total ties:  0\n",
      "Zeros total ties:  0\n",
      "Ones total ties:  0\n",
      "Avg. Time:  0.04245968771595755  seconds.\n",
      "Expected out-sample performance:  0.589561812166633\n",
      "Ones distribution:  0.8660455067179799\n",
      "Ones:  12827 Zeros:  -1984 \n",
      "\n",
      "\n",
      "1000/1000;2;True;30.760247468948364;0.04612355124950409;92.24776649475098;0.04245968771595755;628.8710749149323;2000;1761;0.8805;1000;940;179;0.94;1000;821;60;0.821;14811;8732;0.589561812166633;12827;7879;1131;0.6142511888984175;1984;853;4948;0.42993951612903225;0;0;0;\n",
      "\n",
      "Training with a tupple of size:  5\n",
      "Bleaching is set to:  False \n",
      "\n",
      "Number of classes being trained: 2\n",
      "dict_keys(['0', '1'])\n",
      "['0', '1']\n",
      "Number of training samples for class 0: 1000\n",
      "Number of training samples for class 1: 1000\n",
      "Number of observations:  2000\n",
      "Predicted correctly:  1775\n",
      "Predicted wrongly:  225\n",
      "Predicted zeros:  1225\n",
      "Predicted ones:  775\n",
      "Zeros correct:  1000\n",
      "Ones correct:  775\n",
      "Zeros wrong:  225\n",
      "Ones Wrong:  0\n",
      "Total ties:  572\n",
      "Zeros total ties:  347\n",
      "Ones total ties:  225\n",
      "Avg. Time:  0.016558778166770934  seconds.\n",
      "In-sample performance:  0.8875\n",
      "Ones distribution:  0.5\n",
      "Ones:  1000 Zeros:  -1000\n",
      "\n",
      "\n",
      "Number of observations:  14811\n",
      "Predicted correctly:  6432\n",
      "Predicted wrongly:  8379\n",
      "Predicted zeros:  8993\n",
      "Predicted ones:  5818\n",
      "Zeros correct:  1299\n",
      "Ones correct:  5133\n",
      "Zeros wrong:  7694\n",
      "Ones Wrong:  685\n",
      "Total ties:  5227\n",
      "Zeros total ties:  782\n",
      "Ones total ties:  4445\n",
      "Avg. Time:  0.01759078648980388  seconds.\n",
      "Expected out-sample performance:  0.4342718249949362\n",
      "Ones distribution:  0.8660455067179799\n",
      "Ones:  12827 Zeros:  -1984 \n",
      "\n",
      "\n",
      "1000/1000;5;False;22.59759283065796;0.016558778166770934;33.11948537826538;0.01759078648980388;260.5377404689789;2000;1775;0.8875;1000;775;0;0.775;1000;1000;225;1.0;14811;6432;0.4342718249949362;12827;5133;685;0.40017151321431355;1984;1299;7694;0.6547379032258065;5227;782;4445;\n",
      "\n",
      "Bleaching is set to:  True \n",
      "\n",
      "Number of classes being trained: 2\n",
      "dict_keys(['0', '1'])\n",
      "['0', '1']\n",
      "Number of training samples for class 0: 1000\n",
      "Number of training samples for class 1: 1000\n",
      "Number of observations:  2000\n",
      "Predicted correctly:  1841\n",
      "Predicted wrongly:  159\n",
      "Predicted zeros:  923\n",
      "Predicted ones:  1077\n",
      "Zeros correct:  882\n",
      "Ones correct:  959\n",
      "Zeros wrong:  41\n",
      "Ones Wrong:  118\n",
      "Total ties:  0\n",
      "Zeros total ties:  0\n",
      "Ones total ties:  0\n",
      "Avg. Time:  0.025075441122055054  seconds.\n",
      "In-sample performance:  0.9205\n",
      "Ones distribution:  0.5\n",
      "Ones:  1000 Zeros:  -1000\n",
      "\n",
      "\n",
      "Number of observations:  14811\n",
      "Predicted correctly:  8644\n",
      "Predicted wrongly:  6167\n",
      "Predicted zeros:  5857\n",
      "Predicted ones:  8954\n",
      "Zeros correct:  837\n",
      "Ones correct:  7807\n",
      "Zeros wrong:  5020\n",
      "Ones Wrong:  1147\n",
      "Total ties:  0\n",
      "Zeros total ties:  0\n",
      "Ones total ties:  0\n",
      "Avg. Time:  0.028113931801790615  seconds.\n",
      "Expected out-sample performance:  0.5836202822226724\n",
      "Ones distribution:  0.8660455067179799\n",
      "Ones:  12827 Zeros:  -1984 \n",
      "\n",
      "\n",
      "1000/1000;5;True;22.80980920791626;0.025075441122055054;50.151700258255005;0.028113931801790615;416.396605014801;2000;1841;0.9205;1000;959;118;0.959;1000;882;41;0.882;14811;8644;0.5836202822226724;12827;7807;1147;0.6086380291572464;1984;837;5020;0.421875;0;0;0;\n",
      "\n",
      "Training with a tupple of size:  8\n",
      "Bleaching is set to:  False \n",
      "\n",
      "Number of classes being trained: 2\n",
      "dict_keys(['0', '1'])\n",
      "['0', '1']\n",
      "Number of training samples for class 0: 1000\n",
      "Number of training samples for class 1: 1000\n",
      "Number of observations:  2000\n",
      "Predicted correctly:  1862\n",
      "Predicted wrongly:  138\n",
      "Predicted zeros:  1138\n",
      "Predicted ones:  862\n",
      "Zeros correct:  1000\n",
      "Ones correct:  862\n",
      "Zeros wrong:  138\n",
      "Ones Wrong:  0\n",
      "Total ties:  384\n",
      "Zeros total ties:  246\n",
      "Ones total ties:  138\n",
      "Avg. Time:  0.015918877601623534  seconds.\n",
      "In-sample performance:  0.931\n",
      "Ones distribution:  0.5\n",
      "Ones:  1000 Zeros:  -1000\n",
      "\n",
      "\n",
      "Number of observations:  14811\n",
      "Predicted correctly:  6668\n",
      "Predicted wrongly:  8143\n",
      "Predicted zeros:  8667\n",
      "Predicted ones:  6144\n",
      "Zeros correct:  1254\n",
      "Ones correct:  5414\n",
      "Zeros wrong:  7413\n",
      "Ones Wrong:  730\n",
      "Total ties:  4599\n",
      "Zeros total ties:  693\n",
      "Ones total ties:  3906\n",
      "Avg. Time:  0.014737518848972682  seconds.\n",
      "Expected out-sample performance:  0.4502059280264668\n",
      "Ones distribution:  0.8660455067179799\n",
      "Ones:  12827 Zeros:  -1984 \n",
      "\n",
      "\n",
      "1000/1000;8;False;22.55067276954651;0.015918877601623534;31.838817358016968;0.014737518848972682;218.27856492996216;2000;1862;0.931;1000;862;0;0.862;1000;1000;138;1.0;14811;6668;0.4502059280264668;12827;5414;730;0.4220784283152725;1984;1254;7413;0.6320564516129032;4599;693;3906;\n",
      "\n",
      "Bleaching is set to:  True \n",
      "\n",
      "Number of classes being trained: 2\n",
      "dict_keys(['0', '1'])\n",
      "['0', '1']\n",
      "Number of training samples for class 0: 1000\n",
      "Number of training samples for class 1: 1000\n",
      "Number of observations:  2000\n",
      "Predicted correctly:  1903\n",
      "Predicted wrongly:  97\n",
      "Predicted zeros:  937\n",
      "Predicted ones:  1063\n",
      "Zeros correct:  920\n",
      "Ones correct:  983\n",
      "Zeros wrong:  17\n",
      "Ones Wrong:  80\n",
      "Total ties:  0\n",
      "Zeros total ties:  0\n",
      "Ones total ties:  0\n",
      "Avg. Time:  0.018190397143363953  seconds.\n",
      "In-sample performance:  0.9515\n",
      "Ones distribution:  0.5\n",
      "Ones:  1000 Zeros:  -1000\n",
      "\n",
      "\n",
      "Number of observations:  14811\n",
      "Predicted correctly:  8595\n",
      "Predicted wrongly:  6216\n",
      "Predicted zeros:  5976\n",
      "Predicted ones:  8835\n",
      "Zeros correct:  872\n",
      "Ones correct:  7723\n",
      "Zeros wrong:  5104\n",
      "Ones Wrong:  1112\n",
      "Total ties:  0\n",
      "Zeros total ties:  0\n",
      "Ones total ties:  0\n",
      "Avg. Time:  0.020382990385404146  seconds.\n",
      "Expected out-sample performance:  0.580311930322058\n",
      "Ones distribution:  0.8660455067179799\n",
      "Ones:  12827 Zeros:  -1984 \n",
      "\n",
      "\n",
      "1000/1000;8;True;21.853085041046143;0.018190397143363953;36.382240295410156;0.020382990385404146;301.89349365234375;2000;1903;0.9515;1000;983;80;0.983;1000;920;17;0.92;14811;8595;0.580311930322058;12827;7723;1112;0.602089342792547;1984;872;5104;0.43951612903225806;0;0;0;\n",
      "\n",
      "Training with a tupple of size:  10\n",
      "Bleaching is set to:  False \n",
      "\n",
      "Number of classes being trained: 2\n",
      "dict_keys(['0', '1'])\n",
      "['0', '1']\n",
      "Number of training samples for class 0: 1000\n",
      "Number of training samples for class 1: 1000\n",
      "Number of observations:  2000\n",
      "Predicted correctly:  1895\n",
      "Predicted wrongly:  105\n",
      "Predicted zeros:  1105\n",
      "Predicted ones:  895\n",
      "Zeros correct:  1000\n",
      "Ones correct:  895\n",
      "Zeros wrong:  105\n",
      "Ones Wrong:  0\n",
      "Total ties:  289\n",
      "Zeros total ties:  184\n",
      "Ones total ties:  105\n",
      "Avg. Time:  0.014210214734077454  seconds.\n",
      "In-sample performance:  0.9475\n",
      "Ones distribution:  0.5\n",
      "Ones:  1000 Zeros:  -1000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations:  14811\n",
      "Predicted correctly:  6685\n",
      "Predicted wrongly:  8126\n",
      "Predicted zeros:  8632\n",
      "Predicted ones:  6179\n",
      "Zeros correct:  1245\n",
      "Ones correct:  5440\n",
      "Zeros wrong:  7387\n",
      "Ones Wrong:  739\n",
      "Total ties:  4196\n",
      "Zeros total ties:  611\n",
      "Ones total ties:  3585\n",
      "Avg. Time:  0.014185851272697364  seconds.\n",
      "Expected out-sample performance:  0.4513537235838228\n",
      "Ones distribution:  0.8660455067179799\n",
      "Ones:  12827 Zeros:  -1984 \n",
      "\n",
      "\n",
      "1000/1000;10;False;21.173150300979614;0.014210214734077454;28.42116904258728;0.014185851272697364;210.10833525657654;2000;1895;0.9475;1000;895;0;0.895;1000;1000;105;1.0;14811;6685;0.4513537235838228;12827;5440;739;0.42410540266625085;1984;1245;7387;0.6275201612903226;4196;611;3585;\n",
      "\n",
      "Bleaching is set to:  True \n",
      "\n",
      "Number of classes being trained: 2\n",
      "dict_keys(['0', '1'])\n",
      "['0', '1']\n",
      "Number of training samples for class 0: 1000\n",
      "Number of training samples for class 1: 1000\n",
      "Number of observations:  2000\n",
      "Predicted correctly:  1916\n",
      "Predicted wrongly:  84\n",
      "Predicted zeros:  954\n",
      "Predicted ones:  1046\n",
      "Zeros correct:  935\n",
      "Ones correct:  981\n",
      "Zeros wrong:  19\n",
      "Ones Wrong:  65\n",
      "Total ties:  0\n",
      "Zeros total ties:  0\n",
      "Ones total ties:  0\n",
      "Avg. Time:  0.016328595399856566  seconds.\n",
      "In-sample performance:  0.958\n",
      "Ones distribution:  0.5\n",
      "Ones:  1000 Zeros:  -1000\n",
      "\n",
      "\n",
      "Number of observations:  14811\n",
      "Predicted correctly:  8478\n",
      "Predicted wrongly:  6333\n",
      "Predicted zeros:  6163\n",
      "Predicted ones:  8648\n",
      "Zeros correct:  907\n",
      "Ones correct:  7571\n",
      "Zeros wrong:  5256\n",
      "Ones Wrong:  1077\n",
      "Total ties:  0\n",
      "Zeros total ties:  0\n",
      "Ones total ties:  0\n",
      "Avg. Time:  0.018227753202337778  seconds.\n",
      "Expected out-sample performance:  0.5724123961920194\n",
      "Ones distribution:  0.8660455067179799\n",
      "Ones:  12827 Zeros:  -1984 \n",
      "\n",
      "\n",
      "1000/1000;10;True;21.109006643295288;0.016328595399856566;32.658655405044556;0.018227753202337778;269.9719891548157;2000;1916;0.958;1000;981;65;0.981;1000;935;19;0.935;14811;8478;0.5724123961920194;12827;7571;1077;0.5902393388945194;1984;907;5256;0.4571572580645161;0;0;0;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuple_sizes = [2, 5, 8, 10]\n",
    "#tuple_sizes = [2]\n",
    "training_set_distribuitions = [distribution]\n",
    "#training_set_distribuitions = [[100, 100]]\n",
    "\n",
    "experiment(training_set_distribuitions, tuple_sizes,\n",
    "           training_patterns, training_data_expected_output, training_data_combined,\n",
    "           test_patterns, test_data_expected_output, test_data_combined, \n",
    "           training_set, test_set,\n",
    "           [False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
