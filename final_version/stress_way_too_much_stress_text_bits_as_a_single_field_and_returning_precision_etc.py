# -*- coding: utf-8 -*-
"""stress_way_too_much_stress.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JcwEba9YmwixQTd_clfPgXpSL_IN1pB8
"""

# Source: https://stackoverflow.com/questions/48596521/how-to-read-data-from-google-drive-using-colaboratory-google
"""
! pip install pydrive

# these classes allow you to request the Google drive API
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive 
from google.colab import auth 
from oauth2client.client import GoogleCredentials

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# tabelas_texto_50_e_100_bits.zip
file_id = "1oGIjrtsjI90PxBnTmVJfogGZ_3JgSDHS"
downloaded = drive.CreateFile({"id": file_id})
downloaded.GetContentFile("tabelas_texto_50_e_100_bits.zip")

# tabelas_texto_200_bits.zip
file_id = "1BpdflV0eOitiUbmge7NktZUNpX7MJh5a"
downloaded = drive.CreateFile({"id": file_id})
downloaded.GetContentFile("tabelas_texto_200_bits.zip")

# train.zip
file_id = "1iqTRfp4_MxqV-M1o27KUlMZGzl6TDKbM"
downloaded = drive.CreateFile({"id": file_id})
downloaded.GetContentFile("train.zip")

# test.zip
file_id = "1tJKdG9LVyuP-7HM8vJs2CM_HhM8w72c9"
downloaded = drive.CreateFile({"id": file_id})
downloaded.GetContentFile("test.zip")

# resources.zip
file_id = "1gCZH41pjJt8FmwD-WBZ8GM_L4u_QTGp_"
downloaded = drive.CreateFile({"id": file_id})
downloaded.GetContentFile("resources.zip")

!unzip -o tabelas_texto_200_bits.zip
!unzip -o tabelas_texto_50_e_100_bits.zip
!unzip -o resources.zip
!unzip -o train.zip
!unzip -o test.zip

!mkdir insights

!pip install git+https://github.com/IAZero/wisardpkg
!pip install imblearn"""

import pandas as pd
from matplotlib import pyplot as plt
import numpy as np
import time
from datetime import datetime
#from Wisard import Wisard
import wisardpkg as wp
import math
import sys
import pickle
import gc
import copy
import random

import os
import psutil
process = psutil.Process(os.getpid())

from imblearn.over_sampling import SMOTE

#from google.colab import files

# The preprocessing part is partialy based on the following "kernel" on Kaggle:
# https://www.kaggle.com/jgoldberg/donorschoose-eda-text-classification/notebook

def preprocess(training_dataframe, resources_dataframe):
    #print(training_dataframe.shape)
    #print(resources_dataframe.shape)
    
    #
    total_price = resources_dataframe.quantity * resources_dataframe.price
    resources_dataframe["total_price"] = total_price
    
    # dropping irrelevant columns
    resources_dataframe = resources_dataframe.drop(["description", "price"], axis=1)
    training_dataframe = training_dataframe.drop(["teacher_id"], axis=1)
    
    # grouping resources data by id
    grouped_resources_dataframe = resources_dataframe.groupby("id", as_index=False, sort=False).sum()
    grouped_resources_dataframe
    
    # merging the two dataframes
    cleaned_df = pd.merge(training_dataframe, grouped_resources_dataframe, how="inner", on=["id"])
    
    
    del total_price
    del resources_dataframe
    del training_dataframe
    del grouped_resources_dataframe
    
    # splitting project categories
    
    #cleaned_df[['category_1','category_2']] = cleaned_df['project_subject_categories'].str.replace(", Care & Hunger", "").str.split(', ', 3, expand=True)
    ##print(cleaned_df['project_subject_categories'].str.replace(", Care & Hunger", "").str.split(', ', 3, expand=True))
    
    #cleaned_df[['subcategory_1','subcategory_2']] = cleaned_df['project_subject_subcategories'].str.replace(", Care & Hunger", "").str.split(', ', 3, expand=True)
    
    ##cleaned_df["category_1"] = cleaned_df["category_1"].fillna("Not Informed")
    #cleaned_df["category_2"] = cleaned_df["category_2"].fillna("Not Informed")
    #cleaned_df["subcategory_2"] = cleaned_df["subcategory_2"].fillna("Not Informed")
    
    #cleaned_df["total_price_category"] = pd.cut(
    #    cleaned_df["total_price"], 
    #    bins=[0,100,250,500,1000,16000], 
    #    labels=["0-100","101-250","251-500","501-1000",">1000"]
    #)
    
    #cleaned_df["n_previous_projects"] = pd.cut(
    #    cleaned_df["teacher_number_of_previously_posted_projects"],
    #    bins=[-1,1,5,10,25,50,500],
    #    labels=['0-1','2-5','6-10','11-25','26-50','51+']
    #)
    
    cleaned_df["project_submitted_datetime"] = pd.to_datetime(cleaned_df['project_submitted_datetime'])
    cleaned_df["month"] = cleaned_df['project_submitted_datetime'].dt.month
    cleaned_df["quarter"] = cleaned_df['project_submitted_datetime'].dt.quarter
    
    cleaned_df["teacher_prefix"] = cleaned_df["teacher_prefix"].fillna("unknown")
    
    #cleaned_df["project_essay_1"] = cleaned_df["project_essay_1"].fillna("")
    #cleaned_df["project_essay_2"] = cleaned_df["project_essay_2"].fillna("")
    #cleaned_df["project_essay_3"] = cleaned_df["project_essay_3"].fillna("")
    #cleaned_df["project_essay_4"] = cleaned_df["project_essay_4"].fillna("")
    
    #cleaned_df["merged_essays"] = cleaned_df['project_title'].astype(str) + " " + cleaned_df['project_essay_1'].astype(str) + " " + cleaned_df['project_essay_2'].astype(str) + " " + cleaned_df['project_essay_3'].astype(str) + " " + cleaned_df['project_essay_4'].astype(str)
    #cleaned_df["merged_essays"] = cleaned_df['project_essay_1'].astype(str) + " " + cleaned_df['project_essay_2'].astype(str) + " " + cleaned_df['project_essay_3'].astype(str) + " " + cleaned_df['project_essay_4'].astype(str)
    
    # dropping more columns
    cleaned_df = cleaned_df.drop([
        "project_submitted_datetime", 
        "project_essay_1", 
        "project_essay_2", 
        "project_essay_3", 
        "project_essay_4",
        "quantity",
        #"total_price",
        "project_subject_categories",
        "project_subject_subcategories",
        #"project_essay_1",
        #"project_essay_2",
        #"project_essay_3",
        #"project_essay_4",
        #"teacher_number_of_previously_posted_projects"
        ], 
        axis=1
    )
    
    #cleaned_df = pd.merge(cleaned_df, words_data, how="inner", on=["id"])
    
    print("Dim.:", cleaned_df.shape)
    
    gc.collect()
    
    return cleaned_df

def encode_as_thermometer(value, min_value, max_value, number_of_intervals, output_type = "string"):
    if value < min_value or value > max_value:
        #print("Value out of range.")
        return
    values_range = float(max_value) - float(min_value)
    ##print("values_range", values_range)
    interval_size = values_range / float(number_of_intervals)
    if interval_size == 0:
        number_of_ones = 0
    else:
        number_of_ones = math.floor(round(math.fabs(value - min_value) / interval_size, 2)) # rounds to 2 decimals to solve float precision issues
    
    ##print(number_of_ones, value, interval_size, round(math.fabs(value - min_value) / interval_size, 2))
    
    string = "1" * (number_of_ones) + "0" * (number_of_intervals - number_of_ones)
    
    if output_type == "string":
        return string
    elif output_type == "integer":
        return [int(c) for c in string]
    else:
        #print("Unrecognized output type.")
        return



def calculate_score(X, y):
    #print("Control point 10.5.1")
    #print(process.memory_info()[0])
    
    number_of_correct_predictions = 0
    true_positives = 0 # approved correctly
    false_positives = 0 # approved wrongly
    true_negatives = 0 # reproved correctly
    false_negatives = 0 # reproved wrongly
    
    for i in range(len(X)):
        ##print(len(X), X)
        ##print("Control point 10.5.2")
        ##print(process.memory_info()[0])
        correct = X[i] == y[i]
        
        if correct:
            number_of_correct_predictions = number_of_correct_predictions + 1
            
            if X[i] == "1":
                true_positives = true_positives + 1
            else:
                true_negatives = true_negatives + 1
            
        else:
            if X[i] == "1":
                false_positives = false_positives + 1
            else:
                false_negatives = false_negatives + 1
            
    
    print(number_of_correct_predictions, true_positives, true_negatives, false_positives, false_negatives)
    
    precision = 0.0
    recall = 0.0
    f1_score = 0.0
    
    if true_positives != 0:
        precision = true_positives / (true_positives + false_positives)
        #print(precision)

        recall = true_positives / (true_positives + false_negatives)
        #print(recall)
        
        f1_score = 2 * (precision * recall) / (precision + recall)
        
    else:
        if true_positives == 0 and false_positives == 0 and false_negatives == 0:
            precision = 1.0
            recall = 1.0
            f1_score = 1.0
    
    return {
        "accuracy": float(number_of_correct_predictions) / float(len(X)), 
        "f1_score": f1_score,
        "precision": precision,
        "recall": recall,
        "total_correct": number_of_correct_predictions,
        "true_positives": true_positives,
        "false_positives": false_positives,
        "true_negatives": true_negatives,
        "false_negatives": false_negatives,
    } 

def cross_validate_wisard(tuple_sizes, number_of_folds, training_sample, sample_targets):
    chunk_size = int(len(training_sample) / number_of_folds)
    #print("Chunk size: " + str(chunk_size) + "\n")
    
    #print("Control point 10.1")
    #print(process.memory_info()[0])
    
    training_sample_chunks = [training_sample[x:x+chunk_size] for x in range(0, len(training_sample), chunk_size)]
    target_chunks = [sample_targets[x:x+chunk_size] for x in range(0, len(sample_targets), chunk_size)]
    
    #print("Control point 10.2")
    #print(process.memory_info()[0])
    
    scores_dict = {}
    
    for tuple_size in tuple_sizes:
        
        scores = {
            "accuracy_scores": [],
            "f1_scores": [],
            "precisions": [],
            "recalls": [],
            "total_correct": [],
            "true_positives": [],
            "true_negatives": [],
            "false_positives": [],
            "false_negatives": [],
        }
        for fold in range(number_of_folds):
            #print("Running fold " + str(fold) + ".")
            
            fold_training_sample = []
            fold_training_targets = []
            
            #print("Control point 10.3")
            #print(process.memory_info()[0])
            
            for i in range(len(training_sample_chunks)):
                if i == fold:
                    continue
                #print("Control point 10.3.1")
                #print(process.memory_info()[0])
                fold_training_sample = fold_training_sample + training_sample_chunks[i]
                fold_training_targets = fold_training_targets + target_chunks[i]
                
            #print("Control point 10.4")
            #print(process.memory_info()[0])
            #wisard = Wisard(tupple_size = tuple_size, seed = 3356, bleaching = True)
            #wisard.fit(fold_training_sample, fold_training_targets)
            
            wann = wp.Wisard(tuple_size, bleachingActivated=True, ignoreZero=False)
            #print("Control point 10.4.1")
            #print(process.memory_info()[0])
            wann.train(fold_training_sample, fold_training_targets)
            #print("Trained.")
            
            #print("Control point 10.5")
            #print(process.memory_info()[0])
            
            #score = wisard.score(training_sample_chunks[fold], target_chunks[fold])
            scores_and_other_data = calculate_score(wann.classify(training_sample_chunks[fold]), target_chunks[fold])
            
            scores["accuracy_scores"].append(scores_and_other_data["accuracy"])
            scores["f1_scores"].append(scores_and_other_data["f1_score"])
            scores["precisions"].append(scores_and_other_data["precision"])
            scores["recalls"].append(scores_and_other_data["recall"])
            scores["total_correct"].append(scores_and_other_data["total_correct"])
            scores["true_positives"].append(scores_and_other_data["true_positives"])
            scores["true_negatives"].append(scores_and_other_data["true_negatives"])
            scores["false_positives"].append(scores_and_other_data["false_positives"])
            scores["false_negatives"].append(scores_and_other_data["false_negatives"])
            
            print(tuple_size, "Scores: " + str(scores_and_other_data["accuracy"]) + " " + str(scores_and_other_data["f1_score"]) + ".\n")
            
            #print("Control point 10.6")
            #print(process.memory_info()[0])
            
            del wann
        scores_dict[tuple_size] = scores
            
    return scores_dict

def write_file_lines(output_file_name, contents_dict, header=False):
    if header:
        file = open(output_file_name, "w")
        file.write("algorithm;field_size;file_name;number_of_fields;number_of_observations;number_of_observations_smote;" +
                   "tuple_size;number_of_folds;avg_accuracy;avg_f1_score;avg_precision;avg_recall;avg_total_correct;avg_true_positives;avg_true_negatives;avg_false_positives;avg_false_negatives;" + 
                   "list_of_accuracies;list_of_f1_scores;list_of_precisions;list_of_recalls;list_of_total_corrects;list_of_true_positives;list_of_true_negatives;list_of_false_positives;list_of_false_negatives\n"
                  )
        file.close()
    else:
        
        lines = ""
        
        evaluated_tuple_sizes = sorted(contents_dict["scores_dict"].keys())
        
        for tuple_size in evaluated_tuple_sizes:
            line = ""
            
            line_contents = [
                # Algorithm
                str(contents_dict["algorithm"]) + ";",

                # field size
                str(contents_dict["field_size"]) + ";",

                # file name
                str(contents_dict["file_name"]) + ";",

                # number of fields
                str(contents_dict["number_of_fields"]) + ";",

                # original number of observations
                str(contents_dict["number_of_observations"]) + ";",

                # number of observations after smote oversampling
                str(contents_dict["number_of_observations_smote"]) + ";",

                # tuple size
                str(tuple_size) + ";",

                # number of folds
                str(contents_dict["number_of_folds"]) + ";",

                # average accuracy
                str(sum(contents_dict["scores_dict"][tuple_size]["accuracy_scores"])/contents_dict["number_of_folds"]) + ";",
                
                # average f1_score
                str(sum(contents_dict["scores_dict"][tuple_size]["f1_scores"])/contents_dict["number_of_folds"]) + ";",
                
                # average precision
                str(sum(contents_dict["scores_dict"][tuple_size]["precisions"])/contents_dict["number_of_folds"]) + ";",
                
                # average recall
                str(sum(contents_dict["scores_dict"][tuple_size]["recalls"])/contents_dict["number_of_folds"]) + ";",
                
                # average total_correct
                str(sum(contents_dict["scores_dict"][tuple_size]["total_correct"])/contents_dict["number_of_folds"]) + ";",
                
                # average true positives
                str(sum(contents_dict["scores_dict"][tuple_size]["true_positives"])/contents_dict["number_of_folds"]) + ";",
                
                # average true negatives
                str(sum(contents_dict["scores_dict"][tuple_size]["true_negatives"])/contents_dict["number_of_folds"]) + ";",
                
                # average fakse positives
                str(sum(contents_dict["scores_dict"][tuple_size]["false_positives"])/contents_dict["number_of_folds"]) + ";",
                
                # average false negatives
                str(sum(contents_dict["scores_dict"][tuple_size]["false_negatives"])/contents_dict["number_of_folds"]) + ";",
                
                

                # individual accuracies
                str(contents_dict["scores_dict"][tuple_size]["accuracy_scores"]).replace("[","").replace("]","").replace(" ","") + ";",
                
                # individual f1 scores
                str(contents_dict["scores_dict"][tuple_size]["f1_scores"]).replace("[","").replace("]","").replace(" ","") + ";",
                
                # individual precisions
                str(contents_dict["scores_dict"][tuple_size]["precisions"]).replace("[","").replace("]","").replace(" ","") + ";",
                
                # individual recalls
                str(contents_dict["scores_dict"][tuple_size]["recalls"]).replace("[","").replace("]","").replace(" ","") + ";",
                
                # individual total corrects
                str(contents_dict["scores_dict"][tuple_size]["total_correct"]).replace("[","").replace("]","").replace(" ","") + ";",
                
                # individual true positives
                str(contents_dict["scores_dict"][tuple_size]["true_positives"]).replace("[","").replace("]","").replace(" ","") + ";",
                
                # individual true negatives
                str(contents_dict["scores_dict"][tuple_size]["true_negatives"]).replace("[","").replace("]","").replace(" ","") + ";",
                
                # individual false positives
                str(contents_dict["scores_dict"][tuple_size]["false_positives"]).replace("[","").replace("]","").replace(" ","") + ";",
                
                # individual false negatives
                str(contents_dict["scores_dict"][tuple_size]["false_negatives"]).replace("[","").replace("]","").replace(" ","") + ";",
                
                #
                #str() + ";",


                "\n",
                #str() + ";",
            ]

            for content in line_contents:
                #print(content)
                line = line + content

            lines = lines + line

        print(lines)

        file = open(output_file_name, "a+")
        file.write(lines)
        file.close()
    
    

# load data
train_file_path = 'train.csv'
#test_file_path = 'test.csv'
resources_file_path = 'resources.csv'

# Read data and store in DataFrame
train_data = pd.read_csv(train_file_path, sep=',')
#test_data = pd.read_csv(test_file_path, sep=',')
resources_data = pd.read_csv(resources_file_path, sep=',')

files_and_data = {
        50: {
            "file_names": [
                "compact_Applied_Learning_50.pickle",
                "compact_Health_Sports_50.pickle",
                "compact_History_Civics_50.pickle",
                "compact_Literacy_Language_50.pickle",
                "compact_Math_Science_50.pickle",
                "compact_Music_The_Arts_50.pickle",
                "compact_Special_Needs_50.pickle",
                "compact_Warmth_Care_Hunger_50.pickle",
            ],
            "dataset_per_category": {
                
            }
        },
        100 : {
            "file_names": [
                "compact_Applied_Learning_100.pickle",
                "compact_Health_Sports_100.pickle",
                "compact_History_Civics_100.pickle",
                "compact_Literacy_Language_100.pickle",
                "compact_Math_Science_100.pickle",
                "compact_Music_The_Arts_100.pickle",
                "compact_Special_Needs_100.pickle",
                "compact_Warmth_Care_Hunger_100.pickle",
            ],
            "dataset_per_category": {
                
            }
        },
        200: {
            "file_names": [
                "compact_Applied_Learning_200.pickle",
                "compact_Health_Sports_200.pickle",
                "compact_History_Civics_200.pickle",
                "compact_Literacy_Language_200.pickle",
                "compact_Math_Science_200.pickle",
                "compact_Music_The_Arts_200.pickle",
                "compact_Special_Needs_200.pickle",
                "compact_Warmth_Care_Hunger_200.pickle",
            ],
            "dataset_per_category": {
                
            }
        },
}


field_size_order = [50, 100, 200]
tuple_sizes = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
algorithm = "wisard" 
number_of_folds = 10

scores_per_field_size = {}
output_file_name = "insights/results_experiment" + datetime.now().strftime('%Y%m%d%H%M%S') + ".csv"
write_file_lines(output_file_name, None, header=True)

print("Control point 1")
print(process.memory_info()[0])
for field_size in field_size_order:
    local_td = train_data.copy()
    local_r = resources_data.copy()
    
    #print("Control point 2")
    #print(process.memory_info()[0])
    
    merged_df = preprocess(local_td, local_r)
    
    #print("Control point 3")
    #print(process.memory_info()[0])
    
    scores_per_file = {}
    
    
    for file_name in files_and_data[field_size]["file_names"]:
        print("Current file: " + file_name)
        
        text_data_file = open(file_name, "rb")
        category_text_data = pickle.load(text_data_file)
        
        
        #print("Control point 4")
        #print(process.memory_info()[0])
        
        category_merged_df = pd.merge(merged_df, category_text_data, how="inner", on=["id"])
        
        #print("Control point 5")
        #print(process.memory_info()[0])
        
        category_merged_df['teacher_prefix_code'] = pd.factorize(category_merged_df['teacher_prefix'])[0]
        category_merged_df['project_grade_category_code'] = pd.factorize(category_merged_df['project_grade_category'])[0]
        category_merged_df['school_state_code'] = pd.factorize(category_merged_df['school_state'])[0]
        
        #print("Control point 6")
        #print(process.memory_info()[0])

        #Reordering lines
        category_merged_df = category_merged_df.sample(frac=1, random_state=3356)
        
        columns_list = ["teacher_prefix_code", "project_grade_category_code", "month", "quarter", "teacher_number_of_previously_posted_projects", "total_price", "school_state_code"]
        
        if True: #include_text_data:
            columns_list = columns_list + [col for col in category_merged_df.columns if col.startswith("col")]
        
        print("Control point 7")
        print(process.memory_info()[0])
        
        # Executes oversampling using SMOTE
        oversampled_x, oversampled_y = SMOTE().fit_sample(
            category_merged_df[columns_list], 
            category_merged_df["project_is_approved"])
        
        #print(type(oversampled_x), type(oversampled_y))
        
        #combining lists
        combined_oversampled_lists = list(zip(oversampled_x, oversampled_y))
        
        
        #Reordering lines
        random.seed(3356)
        random.shuffle(combined_oversampled_lists)
        
        
        
        X_resampled, y_resampled = zip(*combined_oversampled_lists)
        X_resampled = np.asarray(X_resampled) 
        y_resampled = np.asarray(y_resampled)
        
        print("Control point 8")
        print(process.memory_info()[0])
        
        print(X_resampled.shape, y_resampled.shape, np.unique(y_resampled, return_counts=True))
        print(X_resampled[0])
        
        training_sample = []
        sample_targets = [str(val) for val in y_resampled]
        
        #print(np.amin(X_resampled, axis=0))
        #print(np.amax(X_resampled, axis=0))
        
        minimums = np.amin(X_resampled, axis=0)
        maximums = np.amax(X_resampled, axis=0)
        
        #print("Control point 9")
        #print(process.memory_info()[0])
        
        for observation in X_resampled:
            int_list = []
            ##print("bla")
            ##print(observation)
            ##print(minimums)
            ##print(maximums)
            
            for i in range(7): # first 7 columns, from the numerical and categorical data
                int_list = int_list + encode_as_thermometer(observation[i], minimums[i], maximums[i], field_size, "integer")
                
            int_list = int_list + [int(column_value) for column_value in observation[7:]] # text data
            ##print(int_list)
            ##print(len(int_list))
            
            training_sample.append(int_list)
            
        #print(training_sample[0])
        #print(len(training_sample), len(training_sample[0]))
        #print("Control point 10")
        #print(process.memory_info()[0])
        
        scores_dict = cross_validate_wisard(tuple_sizes, number_of_folds, training_sample, sample_targets)
        print(scores_dict)
        
        scores_per_file[file_name] = scores_dict
        
        #print("Control point 11")
        #print(process.memory_info()[0])
        
        
        data_to_write = {
            "algorithm": algorithm,
            "field_size": field_size,
            "file_name": file_name,
            "number_of_fields": len(X_resampled[0]) / field_size,
            "number_of_observations": len(category_merged_df),
            "number_of_observations_smote": len(X_resampled),
            "number_of_folds": number_of_folds,
            "scores_dict": scores_dict
        }
        
        write_file_lines(output_file_name, data_to_write)
        
        
        del category_text_data
        del category_merged_df
        del X_resampled
        del y_resampled
        del training_sample
        del sample_targets
        
        gc.collect()
        
        
    scores_per_field_size[field_size] = scores_per_file

print(scores_per_field_size)

#!ls insights
#!cat insights/results_experiment20180909213944.csv

